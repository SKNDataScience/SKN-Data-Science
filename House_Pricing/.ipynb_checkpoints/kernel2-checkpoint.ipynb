{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d4a696348ebc7adae125386edd1ed5388fa0298"
   },
   "source": [
    "<h1>Pierwszy projekt ML</h1>\n",
    "<h2>#TODO podział setu treningowego na testowy i treningowy na własny użytek w celu przetestowania modelu</h2>\n",
    "<h3>Problem regresji</h3>\n",
    "W tym notatniku przejdziemy przez podstawowe kroki załadowania, obróbki i predykcji danych. \n",
    "Kolejność kroków wykonanych w tym notatniku :\n",
    "\n",
    "1.) Analiza danych i działanie na obserwacjach\n",
    "    - Import bibliotek i danych\n",
    "    - Opis danych\n",
    "    - Znalezienie korelacji między atrybutami\n",
    "    - Pozbycie się odstających obserwacji\n",
    "    - Wprowadzenie brakujących danych\n",
    "    #TODO\n",
    "    - Poprawienie atrybutów************************************************\n",
    "    - Dodanie atrybutów\n",
    "    \n",
    "2.) Cechy statystyczne \n",
    "    - Skośność i kurtoza\n",
    "    - Label Encoding\n",
    "    - Transformacja i skalowanie danych\n",
    "    - Wybór atrybutów\n",
    "    - Principal Component Analysis\n",
    "    \n",
    "3.) Dobór modelu i jego ocena\n",
    "    - Testowanie różnych modeli\n",
    "    - Hiperparametryzacja\n",
    "    - Łączenie modeli\n",
    "    - Predykcja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a82eea53f3fd64eda0f064f3dc36951a7f3ae1f9"
   },
   "source": [
    "<h1>1.) Analiza danych i działania na obserwacjach </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a82eea53f3fd64eda0f064f3dc36951a7f3ae1f9"
   },
   "source": [
    "<h1>Import bibliotek i danych</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ff9096edbc92c0ffca257ae73928951bc544de7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # processing danych\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt # wykresy\n",
    "%matplotlib inline\n",
    "import seaborn as sns # wizualizacja danych\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import scipy.stats as st # moduł statystyczny\n",
    "pd.options.display.max_columns = None # show all columns\n",
    "# import missingno as msno # missing data visualizations and utilities\n",
    "import warnings # ignore file warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importing the train and test datasets in  pandas dataframe\n",
    "\n",
    "trainData = pd.read_csv(\"C:/Users/ksmoc/OneDrive/Workspace/PycharmProjects/ML_Projects/House_Pricing/data/train.csv\")\n",
    "testData = pd.read_csv(\"C:/Users/ksmoc/OneDrive/Workspace/PycharmProjects/ML_Projects/House_Pricing/data/test.csv\")\n",
    "trainData.drop(columns = 'Id', inplace =True)\n",
    "y_train = trainData['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ed0632bede7e3a01dfe757b610599de83014fc34"
   },
   "source": [
    "<h1>Wstępna charakterystyka danych</h1>\n",
    "Spójrzmy na dane co mamy i jaki jest ich ogólny obraz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ea22d699eab4bcc9b7d181a2bf0164e61a512a0"
   },
   "outputs": [],
   "source": [
    "# Krztałt danych (wymiary tablic)\n",
    "trainData.shape, testData.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c6d6b2cda812e570cf7270ec0f5467979153880"
   },
   "outputs": [],
   "source": [
    "# Podstawowy opis zbioru danych\n",
    "trainData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d5c7a2a4717cece181f481499818398202120ec6"
   },
   "outputs": [],
   "source": [
    "# Pierwsze pięć kolumn zbioru\n",
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "839b7417f336eb4592cbd2277d68fab87d6b6381"
   },
   "outputs": [],
   "source": [
    "# Ostatnie pięć kolumn zbioru\n",
    "trainData.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0034b04d8dd1f4e2ed5bf432cf0bf5543cc6820b"
   },
   "outputs": [],
   "source": [
    "# Atrybuty w zbiorze danych zawierające dane numeryczne \n",
    "numerical_features = trainData.select_dtypes(include=[np.number]) \n",
    "numerical_features.columns\n",
    "\n",
    "# Atrybuty w zbiorze danych zawierające dane kategorialne\n",
    "categorical_features = trainData.select_dtypes(include=[np.object])\n",
    "categorical_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5fc916932d2b9491a2358e3f06814b49cd94a50f",
    "scrolled": true
   },
   "source": [
    "<h1>Znalezienie korelacji między atrybutami</h1>\n",
    "Spojrzymy jak się rozkładają korelacje pomiędzy poszczególnymi atrybutami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "827d41628563f4c77743e0862b55d1e82d0184af"
   },
   "outputs": [],
   "source": [
    "# Spróbujemy znaleźć atrybuty posiadające największą korelacje (czyli te od których najbardziej zależy) \n",
    "# szukana przez nas wartość 'SalePrice', posortowane od największej do najmniejszej.\n",
    "\n",
    "correlation = numerical_features.corr()\n",
    "print(correlation['SalePrice'].sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d83ff8e4cad9e61520c477426270412998f67d30"
   },
   "outputs": [],
   "source": [
    "# Mapa cieplna korelacji atrybutów numerycznych\n",
    "f , ax = plt.subplots(figsize = (14,12))\n",
    "plt.title('Korelacja atrybutów numerycznych',size=15)\n",
    "sns.heatmap(correlation,square = True,  vmax=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "733e6f6ebfe75a90046bcc219cc3c53c17398754"
   },
   "source": [
    "- Widzimy dwa białe kwadraty (2,2 and 3,3) Które wskazują na wysoką korelację. Pierwsza grupa silnie skorelowanych atrybutów to 'TotalBsmtSF' i '1stFlrSF'. Druga grupa to 'GarageYrBlt', 'GarageCars' i 'GarageArea'. To oznacza wieloliniowość.\n",
    "- Inne cztery białe kwadraty (1,1) wskazują na oczywistą korelację między 'GarageYrBlt' i 'YearBuilt' oraz między 'TotRmsAbvGrd' i 'GrLivArea'\n",
    "- Ponadto z mapy cieplnej i poprzedniej oceny korelacji odczytujemy, że'GrLivArea', 'TotalBsmtSF', 'OverallQual', 'FullBath', 'TotRmsAbvGrd' oraz 'YearBuilt' są silnie skorelowane z 'SalePrice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0d86a4699cdb709983c08abc41fd87b62924016"
   },
   "outputs": [],
   "source": [
    "# Zbliżenie mapy cieplnej najbardziej skorelowanych atrybutów\n",
    "zoomedCorrelation = correlation.loc[['SalePrice','GrLivArea','TotalBsmtSF','OverallQual','FullBath','TotRmsAbvGrd','YearBuilt','1stFlrSF','GarageYrBlt','GarageCars','GarageArea'], ['SalePrice','GrLivArea','TotalBsmtSF','OverallQual','FullBath','TotRmsAbvGrd','YearBuilt','1stFlrSF','GarageYrBlt','GarageCars','GarageArea']]\n",
    "f , ax = plt.subplots(figsize = (14,12))\n",
    "plt.title('Korelacja atrybutów numerycznych',size=15)\n",
    "sns.heatmap(zoomedCorrelation, square = True, linewidths=0.01, vmax=0.8, annot=True,cmap='viridis',\n",
    "            linecolor=\"black\", annot_kws = {'size':12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "662c66a3ab59f7b186073c174b7c740633017b52"
   },
   "source": [
    "Stwierdzamy, że:\n",
    "- 'TotalBsmtSF' oraz '1stFlrSF' są silnie skorelowane\n",
    "- 'TotRmsAbvGrd' oraz 'GrLivArea' są silnie skorelowane\n",
    "- 'GarageCars' oraz 'GarageArea' są silnie skorelowane\n",
    "- 'GarageYrBlt' oraz 'YearBuilt' są silnie skorelowane\n",
    "- 'TotRmsAbvGrd' oraz 'GrLivArea' są silnie skorelowane\n",
    "- 'OverallQual', 'GrLivArea' i 'TotRmsAbvGrd' są silnie skorelowane z 'SalePrice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5ef1ee7de4088f1c747d275fb5a33ac5921ed49f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wykresy dwóch zmiennych\n",
    "sns.set()\n",
    "cols = ['SalePrice','GrLivArea','TotalBsmtSF','OverallQual','FullBath','TotRmsAbvGrd','YearBuilt','1stFlrSF','GarageYrBlt','GarageCars','GarageArea']\n",
    "sns.pairplot(trainData[cols],size = 2 ,kind ='scatter',diag_kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8098d9919d828cdf84a656f9fb3af1802786ad23"
   },
   "source": [
    "- Widzimy, że 'SalePrice' wzrasta kwadratowo wraz z wzrostem wartości 'TotalBsmtSF', 'GrLivArea', '1stFlrSF'. Wnioskujemy z tego, że cena domu zwiększa się o kwadrat przyrostu powierzchni. Ponadto widzimy, że 'SalePrice' wzrasta wykładniczo wraz z 'OverallQual'.\n",
    "- Ponadto z 'GrLivArea'-'1stFlSF' oraz '1stFlSF'-'TotalBsmSF' obserwujemy, że wszystkie punkty znajdują się powyżej lini funkcji tożsamościowej, co oznacza, że parter posiada większą powierzchnie niż którekolwiek z pięter, oraz, że pierwsze piętro jest większe niż piwnica.\n",
    "- Podobne zjawisko zachodzi dla 'GarageYrBlt'-'YearBuilt' co ma sens ponieważ przeważnie najpierw budujemy dom a dopiero następnie garaż, jednakże zachodzą tutaj pewne wyjątki w naszym zbiorze danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ee5814221e5558f50204521048704955efc038d9"
   },
   "source": [
    "<h1>Pozbycie się odstających obserwacji</h1>\n",
    "\n",
    "Z poprzednich wykresów dwóch zmiennych widzimy kilka obserwacji odstających dla 'TotalBsmtSF', '1stFlrSF' oraz 'GrLivArea'. Skorzystamy z wykresu punktowego aby zobaczyć je bardziej dokładnie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65930a0a46dfd13edd48abcf57a51ed7bdae32f3"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(x = trainData.TotalBsmtSF,y = trainData.SalePrice)\n",
    "plt.title('TotalBsmtSF', size = 15)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(x = trainData['1stFlrSF'],y = trainData.SalePrice)\n",
    "plt.title('1stFlrSF', size = 15)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(x = trainData.GrLivArea,y = trainData.SalePrice)\n",
    "plt.title('GrLivArea', size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f2cf7d0f80198739c32fcb52188400b6e9c2ea5"
   },
   "outputs": [],
   "source": [
    "# Usuwanie obserwacji odstających\n",
    "trainData.drop(trainData[trainData['TotalBsmtSF'] > 5000].index,inplace = True)\n",
    "trainData.drop(trainData[trainData['1stFlrSF'] > 4000].index,inplace = True)\n",
    "trainData.drop(trainData[(trainData['GrLivArea'] > 4000) & (trainData['SalePrice']<300000)].index,inplace = True)\n",
    "trainData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b07ef0cb20fb2bde69af7902614292542734a873"
   },
   "source": [
    "Ponieważ odrzuciliśmy tylko dwie obserwacje odstające oznacza to, że wszystkie trzy cechy dzieliły tą samą obserwację."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21775dc34f04cdcdf20e100631cd96c6cc70e4bd"
   },
   "source": [
    "<h1>Wprowadzenie brakujących danych</h1>\n",
    "\n",
    "Teraz przyjrzymy się brakujących danych w naszym zbiorze.\n",
    "Będziemy korzystać z biblioteki msno (missingno). Msno zapewnia mały zestaw narzędzi do wizualizacji brakujących danych oraz funkcjonalności które pozwalają na szybkie wizualne podsumowanie kompletności danych albo jej braku w twoim zbiorze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5bbdd27fbabb40f4e65ad414f73e5856dee76791"
   },
   "outputs": [],
   "source": [
    "# Visualising missing values of numeric features for sample of 200\n",
    "msno.matrix(trainData.select_dtypes(include=[np.number]).sample(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b962cdac0c83c722d68076420720f97880a0811"
   },
   "outputs": [],
   "source": [
    "# Visualising percentage of missing values of the top 10 numeric variables\n",
    "total = trainData.select_dtypes(include=[np.number]).isnull().sum().sort_values(ascending=False)\n",
    "percent = (trainData.select_dtypes(include=[np.number]).isnull().sum()/trainData.select_dtypes(include=[np.number]).isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1,join='outer', keys=['Missing Count', 'Missing Percentage'])\n",
    "missing_data.index.name =' Numeric Feature'\n",
    "missing_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4e68257bbee51ef367869a62305fda0fb75fe949"
   },
   "source": [
    "We observe that 'LotFrontage', 'GarageYrBlt' and 'MasVnrArea' are the only one who have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4e4c57e9f1e220bb204d14deeaae7264740dfc02"
   },
   "outputs": [],
   "source": [
    "# Wizualizacja brakujących danych z atrybutów o kategorialnych danych w próbie dwustu obserwacji.\n",
    "msno.matrix(trainData.select_dtypes(include=[np.object]).sample(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "729e87134c4f6884881eca0dfd2632a7fe50b542"
   },
   "outputs": [],
   "source": [
    "# Wizualizacja procentowego udziału brakujących obserwacji w top dziesięciu atrybutach zawierających brakujące dane kategorialne\n",
    "total = trainData.select_dtypes(include=[np.object]).isnull().sum().sort_values(ascending=False)\n",
    "percent = (trainData.select_dtypes(include=[np.object]).isnull().sum()/trainData.select_dtypes(include=[np.object]).isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1,join='outer', keys=['Missing Count', 'Missing Percentage'])\n",
    "missing_data.index.name =' Numeric Feature'\n",
    "missing_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4e73c182fabf41d2c80ccc6ea1bdf8bc073fc869"
   },
   "source": [
    "Okazuje się, że 'PoolQC', 'MiscFeature', 'Alley', 'Fence' oraz 'FireplaceQu' posiadają znaczącą ilość brakujących danych (przynajmniej połowa obserwacji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e1b61c4094acf40c399f1e9422ecd255ef0333c"
   },
   "outputs": [],
   "source": [
    "# Wizualizacja wybrakowania według kolumny\n",
    "msno.bar(trainData.sample(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7bf3cabc506c99a6c3dfa8a62a9d673a78335eae"
   },
   "outputs": [],
   "source": [
    "# Mapa cieplna korelacji wybrakowania tz. jak bardzo obecność lub brak pewnej obserwacji wpływa na inną\n",
    "msno.heatmap(trainData)\n",
    "\n",
    "# -1 : jeżeli jedna obserwacja jest obecna drugiej na pewno nie ma\n",
    "# 0 : obecność obserwacji lub jej brak nie ma wpływu na inną obserwację  \n",
    "# 1 : jeżeli jedna obserwacja jest obecna druga na pewno też jest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0003a9c144b48e5947564ade8e645b4e0aa6338c"
   },
   "outputs": [],
   "source": [
    "# Dendrogram kompletności obserwacji, pokazuje trendy korelacyjne między obserwacjami głębsze niż te wynikające z mapy cieplnej.\n",
    "msno.dendrogram(trainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d66d4b26e71e56b513c61f431e665eb97cd2d6e8"
   },
   "source": [
    "Liście klastra które są połączone ze sobą z zerową odległością, w pełni określają swoją wzajemną obecność: jedna obserwacja może być zawsze pusta jeżeli druga istnieje, lub obie mogą być zawsze obecne lub puste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "258bedf0317ae25d033c39c74955fc992be32476"
   },
   "source": [
    "Rozpoczniemy od zastępowania brakujących danych w zbiorach testowych i treningowych. \n",
    "W tym celu najpierw połączymy je w jeden zbiór."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b8b70432addea1eedf18a4f53a56cdfa880b8314"
   },
   "outputs": [],
   "source": [
    "# Powiążemy zbiory treningowy i testowy w jeden obiekt Dataframe\n",
    "dataFull = pd.concat([trainData,testData],ignore_index=True)\n",
    "dataFull.drop('Id',axis = 1,inplace = True)\n",
    "dataFull.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ed11477dc4b68adf69e46ae247d17b95abcfe3f7"
   },
   "outputs": [],
   "source": [
    "# Suma brakujących obserwacji w zależności od atrybutu\n",
    "sumMissingValues = dataFull.isnull().sum()\n",
    "sumMissingValues[sumMissingValues>0].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8879dc08f43263c8a541b9b4fad8745353525257"
   },
   "outputs": [],
   "source": [
    "# Atrybuty numeryczne: zastępujemy zerem. Dlaczego akurat te?\n",
    "for col in ['BsmtFullBath','BsmtHalfBath','BsmtUnfSF','TotalBsmtSF','GarageCars','BsmtFinSF2','BsmtFinSF1','GarageArea']:\n",
    "    dataFull[col].fillna(0,inplace= True)\n",
    "\n",
    "# Sprawdzamy czy udało nam się je zastąpić.\n",
    "dataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22a5dadab1dc80097fb28f13d9c668cdce89f32b"
   },
   "source": [
    "Rozpoczniemy od atrybutów które posiadają mniej niż 5 brakujących obserwacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a1dc53d6e043d1c5421ae997589559c40694491b"
   },
   "outputs": [],
   "source": [
    "# Atrybuty kategorialne: zastępujemy modalną (najczęściej występującą wartością)\n",
    "for col in ['MSZoning','Functional','Utilities','KitchenQual','SaleType','Exterior2nd','Exterior1st','Electrical']:\n",
    "    dataFull[col].fillna(dataFull[col].mode()[0],inplace= True)\n",
    "\n",
    "# Sprawdzamy czy udało nam się je zastąpić.\n",
    "dataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "49959c4f2b541d23f6b1f80638f1d88811afc523"
   },
   "outputs": [],
   "source": [
    "# Przypisujemy atrybuty które posiadają więcej niż 5 brakujących obserwacji.\n",
    "\n",
    "# Dane kategorialne: Zmieniamy wszystkie na \"None\"\n",
    "for col in ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','GarageQual','GarageCond','GarageFinish','GarageType','BsmtExposure','BsmtCond','BsmtQual','BsmtFinType2','BsmtFinType1','MasVnrType']:\n",
    "    dataFull[col].fillna('None',inplace = True)\n",
    "\n",
    "# Sprawdzamy czy udało nam się je zastąpić.\n",
    "dataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "432c8b82e92c1a69cbd60b2a6b794d9e59fcd00c"
   },
   "source": [
    "Ponieważ 'MasVnrArea' posiada tylko 23 brakujące obserwacje, możemy zastąpić je średnią dla kolumny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42df6bc540f462d5a953b1271ec4a1bb8ee54a87"
   },
   "outputs": [],
   "source": [
    "dataFull['MasVnrArea'].fillna(dataFull['MasVnrArea'].mean(), inplace=True)\n",
    "\n",
    "# Sprawdzamy czy udało nam się je zastąpić.\n",
    "dataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4c8bce62652c1da6aa6162af793a27c30c9d29de"
   },
   "source": [
    "Bazując na mapie cieplnej korelacji wiemy, że 'GarageYrBlt' jest silnie skorelowane z 'YearBuilt'. Z tego powodu zastąpimy brakujące wartości medianami z 'YearBuilt'. \n",
    "\n",
    "Z tego względu iż atrybut 'YearBuilt' zawiera dane numeryczne musimy podzielić go na przedziały."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80ed1605561a318eaa11710470a98a25f495f72b"
   },
   "outputs": [],
   "source": [
    "# Dzielimy 'YearBuilt' na 10 przedziałów\n",
    "dataFull['YearBuiltCut'] = pd.qcut(dataFull.YearBuilt,10)\n",
    "# Zastąpienie brakujących obserwacji atrybutu 'GarageYrBlt' bazując na medianie atrybutu 'YearBuilt' \n",
    "dataFull['GarageYrBlt']= dataFull.groupby(['YearBuiltCut'])['GarageYrBlt'].transform(lambda x : x.fillna(x.median()))\n",
    "# Rzutowanie typu na liczbowy (int)\n",
    "dataFull['GarageYrBlt'] = dataFull['GarageYrBlt'].astype(int)\n",
    "# Usuwamy kolumnę 'YearBuiltCut'\n",
    "dataFull.drop('YearBuiltCut',axis=1,inplace=True)\n",
    "# # Sprawdzamy czy udało nam się zastąpić brakujące obserwacje.\n",
    "dataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8a458b6a14f7d5ad5efb4f05a88a50a1ad6bc3a6"
   },
   "source": [
    "Na podstawie mapy cieplnej korelacji wiemy że, 'LotFrontage' jest silnie skorelowane z 'LotArea' oraz 'Neighbourhood'. \n",
    "Dokonamy tego samego co w przypadku 'YearBuilt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b410ffb9b83df27ba91cd48aa95bc31c9cd670e"
   },
   "outputs": [],
   "source": [
    "# Dzielimy atrybut 'LotArea' na 10 przedziałów\n",
    "dataFull['LotAreaCut'] = pd.qcut(dataFull.LotArea,10)\n",
    "\n",
    "# Zastępujemy brakujące obserwacje atrybutu 'LotFrontage' opierając się na medianie atrybutów 'LotArea' oraz 'Neighbourhood'\n",
    "dataFull['LotFrontage']= dataFull.groupby(['LotAreaCut','Neighborhood'])['LotFrontage'].transform(lambda x : x.fillna(x.median()))\n",
    "dataFull['LotFrontage']= dataFull.groupby(['LotAreaCut'])['LotFrontage'].transform(lambda x : x.fillna(x.median()))\n",
    "\n",
    "# Usuwamy kolumnę 'LotAreaCut'\n",
    "dataFull.drop('LotAreaCut',axis=1,inplace=True)\n",
    "\n",
    "# Sprawdzamy czy udało nam się zastąpić brakujące obserwacje.\n",
    "dataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa810252be324846d80c9176a3b8cc46ff089186"
   },
   "source": [
    "Jedyne pozostałe brakujące obserwacje należą do atrybutu 'Sale price', który odzwierciedla liczbę obserwacji z testowego zbioru danych, które musimy przewidzieć."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4b401eed6bdfab152ef798a8e3c7a9985ca13f91"
   },
   "source": [
    "<h1># TODO Correcting Features</h1>\n",
    "\n",
    "If we take a look at the numeric variables, we see that some of them obviously don't make a sense being numerical like year related features. Let's take a closer look at each one of them in the data description file and see which ones need to be converted to categorical type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb6ff2bc808748a0e6139e0534beacddfb035b15"
   },
   "outputs": [],
   "source": [
    "dataFull.select_dtypes(include=[np.number]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a92b0609c827750d6c5a9b234996a82773146c58"
   },
   "outputs": [],
   "source": [
    "# Converting numeric features to categorical features\n",
    "strCols = ['YrSold','YearRemodAdd','YearBuilt','MoSold','MSSubClass','GarageYrBlt']\n",
    "for i in strCols:\n",
    "    dataFull[i]=dataFull[i].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3d96f82c952e3da9f69fbcd9ce6714b10ae7751c"
   },
   "source": [
    "<h1>Adding Features</h1>\n",
    "\n",
    "First, we will map some categorical variable that represent some sort of rating to an integer score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2bcf072c4469f089d17f8ec2fb29ea1ad13923ae"
   },
   "outputs": [],
   "source": [
    "dataFull.select_dtypes(include=[np.object]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de518dcf63db554aa5bc645b4df6123918f0ed9a"
   },
   "outputs": [],
   "source": [
    "dataFull[\"oExterQual\"] = dataFull.ExterQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n",
    "dataFull[\"oBsmtQual\"] = dataFull.BsmtQual.map({'None':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n",
    "dataFull[\"oBsmtExposure\"] = dataFull.BsmtExposure.map({'None':1, 'No':2, 'Av':3, 'Mn':3, 'Gd':4})\n",
    "dataFull[\"oHeatingQC\"] = dataFull.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n",
    "dataFull[\"oKitchenQual\"] = dataFull.KitchenQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n",
    "dataFull[\"oFireplaceQu\"] = dataFull.FireplaceQu.map({'None':1, 'Po':2, 'Fa':3, 'TA':4, 'Gd':5, 'Ex':6})\n",
    "dataFull[\"oGarageFinish\"] = dataFull.GarageFinish.map({'None':1, 'Unf':2, 'RFn':3, 'Fin':4})\n",
    "dataFull[\"oPavedDrive\"] = dataFull.PavedDrive.map({'N':1, 'P':2, 'Y':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "641998bc7f20ff6d6d56ae7d88c232be45de4440"
   },
   "source": [
    "Next, we will add up some numeric features with each other to create new features that make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7cc5a42e0dd33fbe3de7a422e63a0647dd5fbf51"
   },
   "outputs": [],
   "source": [
    "dataFull.select_dtypes(include=[np.number]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce90331007d43c7c910fde509a01b114f65f9b34"
   },
   "outputs": [],
   "source": [
    "dataFull['HouseSF'] = dataFull['1stFlrSF'] + dataFull['2ndFlrSF'] + dataFull['TotalBsmtSF']\n",
    "dataFull['PorchSF'] = dataFull['3SsnPorch'] + dataFull['EnclosedPorch'] + dataFull['OpenPorchSF'] + dataFull['ScreenPorch']\n",
    "dataFull['TotalSF'] = dataFull['HouseSF'] + dataFull['PorchSF'] + dataFull['GarageArea']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa177559dba12cc10a373cf1bd7ad3b96b8ac737",
    "scrolled": false
   },
   "source": [
    "<h1> 2.) Cechy statystyczne </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa177559dba12cc10a373cf1bd7ad3b96b8ac737",
    "scrolled": false
   },
   "source": [
    "<h1>Skewness and Kurtosis</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "833fc77ce186c9a88ecb6c9f89857c7f950a2919"
   },
   "outputs": [],
   "source": [
    "# Estimate Skewness and Kurtosis of the data\n",
    "trainData.skew(), trainData.kurt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a239c77acd942652777773e39c9ae53783c65ae8"
   },
   "outputs": [],
   "source": [
    "# Plot the Skewness of the data\n",
    "sns.distplot(trainData.skew(),axlabel ='Skewness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f3717c5937a44da83805dd054c1e9beea929acf"
   },
   "outputs": [],
   "source": [
    "# Plot the Kurtosis of the data\n",
    "sns.distplot(trainData.kurt(),axlabel ='Kurtosis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c68e3cfc8990f3f8108ac43f205b9fe4bc4ce8d"
   },
   "source": [
    "There isn't much kurtosis in the data columns, but Skewness is very present, meaning that distribution is not symetrical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "171ea855e52fd4637aa5fede4400f1e75568b012"
   },
   "source": [
    "<h1>Label Encoding</h1>\n",
    "For this section we will use Pipelines which are a way to streamline a lot of the routine processes. It provides a way to take code, fit it to the training data, apply it to the test data without having to copy and paste everything.\n",
    "We will create three classes : first for label encoding, second for skewness, and third for one hot label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0035df3ce0837c4483f3ae938b3a7bc4392d9c62"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer\n",
    "from scipy.stats import skew\n",
    "\n",
    "# Label encoding class\n",
    "class labenc(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        label = LabelEncoder()\n",
    "        X['YrSold']=label.fit_transform(X['YrSold'])\n",
    "        X['YearRemodAdd']=label.fit_transform(X['YearRemodAdd'])\n",
    "        X['YearBuilt']=label.fit_transform(X['YearBuilt'])\n",
    "        X['MoSold']=label.fit_transform(X['MoSold'])\n",
    "        X['GarageYrBlt']=label.fit_transform(X['GarageYrBlt'])\n",
    "        return X\n",
    "    \n",
    "# Skewness transform class\n",
    "class skewness(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        skewness = X.select_dtypes(include=[np.number]).apply(lambda x: skew(x))\n",
    "        skewness_features = skewness[abs(skewness) >= 1].index\n",
    "        X[skewness_features] = np.log1p(X[skewness_features])\n",
    "        return X\n",
    "\n",
    "# One hot encoding class\n",
    "class onehotenc(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        X = pd.get_dummies(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0dc7638a3fa042e1f84a5aa59773c25ebd487794"
   },
   "outputs": [],
   "source": [
    "# Creating a copy of the full dataset\n",
    "dataFullCopy = dataFull.copy()\n",
    "\n",
    "# Creating a new fata with aplied transformations using sklearn Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([('labenc',labenc()),('skewness',skewness()),('onehotenc',onehotenc())])\n",
    "dataPipeline = pipeline.fit_transform(dataFullCopy)\n",
    "dataFull.shape, dataPipeline.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e00a3493f46d3f2c47d369e2ee88eb0c5b5516c6"
   },
   "source": [
    "We can see now that the number of features increases from 88 to 328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0b2918d974c03c24ac246e4881bd5764ef98747"
   },
   "outputs": [],
   "source": [
    "dataPipeline.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a008941b3541648ddc2b29010302a247d817b569"
   },
   "source": [
    "Now we split the data to training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a290d531b0212f3f23ddd4b18072f984ad989282"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = dataPipeline[:trainData.shape[0]]\n",
    "y_train = X_train['SalePrice']\n",
    "X_train.drop(columns = 'SalePrice', inplace=True)\n",
    "X_test = dataPipeline[trainData.shape[0]:]\n",
    "X_test.drop(columns = 'SalePrice', inplace=True)\n",
    "X_train.shape, y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab3fe8b236322172a6eccd1fce1a9361186a2677",
    "scrolled": false
   },
   "source": [
    "<h1>Transformation and Scaling</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1d84d6dd76571128298d744512566a91dd9fa29"
   },
   "outputs": [],
   "source": [
    "# SalesPrices plot with three different fitted distributions\n",
    "plt.figure(2); plt.title('Normal')\n",
    "sns.distplot(y_train, kde=False, fit=st.norm)\n",
    "plt.figure(3); plt.title('Log Normal')\n",
    "sns.distplot(y_train, kde=False, fit=st.lognorm)\n",
    "plt.figure(1); plt.title('Johnson SU')\n",
    "sns.distplot(y_train, kde=False, fit=st.johnsonsu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0fd07fe2f228ef06a852c95c566a7122bc02e0aa"
   },
   "source": [
    "Normal distribution doesn't fit, so SalePrice need to be transformed before creating the model. Best fit is unbounded Johnson distribution, altough log normal distribution also fits well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "75cab4022a761e59260aa7fbd593e30d17f6645f"
   },
   "outputs": [],
   "source": [
    "# transforming 'SalePrice' into normal distribution\n",
    "y_train_transformed = np.log(y_train)\n",
    "y_train_transformed.skew(), y_train_transformed.kurt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e705f5af6fd6e7746b410d47548355bdea0c4ba2"
   },
   "outputs": [],
   "source": [
    "# plotting 'SalePrice' before and after the transformation\n",
    "plt.figure(1); plt.title('Before transformation')\n",
    "sns.distplot(y_train)\n",
    "plt.figure(2); plt.title('After transformation')\n",
    "sns.distplot(y_train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "36daa9272d2c35b42ecfc4fd71c05913691027b5"
   },
   "outputs": [],
   "source": [
    "# Using Robust Scaler to transform X_train\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "X_train_scaled = robust_scaler.fit(X_train).transform(X_train)\n",
    "X_test_scaled = robust_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b6507ef8380f6ea9611f0a308a04af06df8916b3"
   },
   "outputs": [],
   "source": [
    "# Shape of final data we will be working on\n",
    "X_train_scaled.shape, y_train_transformed.shape, X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14c016f3b72983c454a97f91cacc6ee34e4e3448"
   },
   "source": [
    "<h1>Feature Selection</h1>\n",
    "\n",
    "We will use lasso regression (l1 regularization method). Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. We can also use it to find the most important features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9d3f3bafb013cf0a4e3547b0fb90578b27afb8ac"
   },
   "outputs": [],
   "source": [
    "# Display features by their importance (lasso regression coefficient)\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha = 0.001)\n",
    "lasso.fit(X_train_scaled,y_train_transformed)\n",
    "y_pred_lasso = lasso.predict(X_test_scaled)\n",
    "lassoCoeff = pd.DataFrame({\"Feature Importance\":lasso.coef_}, index=dataPipeline.drop(columns = 'SalePrice').columns)\n",
    "lassoCoeff.sort_values(\"Feature Importance\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "82c31d3b13a9b0e6de1b50fb85730240db9fd807"
   },
   "outputs": [],
   "source": [
    "# Plot features by importance (feature coefficient in the model)\n",
    "lassoCoeff[lassoCoeff[\"Feature Importance\"]!=0].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(20,35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "69138124ef97484291ab98c0f0e3a436d8a1d7a7"
   },
   "source": [
    "What's intersting here is that two of the variables that we have created 'HouseSF' and 'PorchSF' perform actually bad compared to their components. But when we sum all the surfaces as in 'TotalSF', which is just a combination of features that are significantly unimportant in this model, we suddently obtain the most important feature in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "033ab832fd50c52b708361f58ee5e994fd48f31f"
   },
   "source": [
    "<h1>Principal Component Analysis</h1>\n",
    "\n",
    "PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5d499e1e37f32a931b9343acb9668f7fffdb80d"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Concatenate the training and test datasets into a single datafram\n",
    "dataFull2 = np.concatenate([X_train_scaled,X_test_scaled])\n",
    "# Choose the number of principle components such that 95% of the variance is retained\n",
    "pca = PCA(0.95)\n",
    "dataFull2 = pca.fit_transform(dataFull2)\n",
    "varPCA = np.round(pca.explained_variance_ratio_*100, decimals = 1)\n",
    "# Principal Component Analysis of data\n",
    "print(varPCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ca59999f82f7aa440c49949879463d37d5a9cf3"
   },
   "outputs": [],
   "source": [
    "# Principal Component Analysis plot of the data\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.bar(x=range(1,len(varPCA)+1), height = varPCA)\n",
    "plt.ylabel(\"Explained Variance (%)\", size = 15)\n",
    "plt.xlabel(\"Principle Components\", size = 15)\n",
    "plt.title(\"Principle Component Analysis Plot : Training Data\", size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52e1fd9f4b9356718a53b193b5f8363138322d3e"
   },
   "outputs": [],
   "source": [
    "# Shape of final data we will be working on\n",
    "X_train_scaled = dataFull2[:trainData.shape[0]]\n",
    "X_test_scaled = dataFull2[trainData.shape[0]:]\n",
    "X_train_scaled.shape, y_train_transformed.shape, X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "42a5e5909c34efa60417701fb7385685176844e5"
   },
   "source": [
    "We see that now we have 87 features instead of the 327 features that we had before using PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ddbb87ef82641537fcbc2ac1a4a053124f85938f"
   },
   "source": [
    "<h1>Testing Different Models</h1>\n",
    "\n",
    "Now that we have finished preparing our data, it's time to test different models to see which one performs the best.\n",
    "The models we will be testing are : \n",
    "- Linear regression\n",
    "- Support vector regression\n",
    "- Stochastic gradient descent\n",
    "- Gradient boosting tree\n",
    "- Random forest\n",
    "- Lasso regression\n",
    "- Ridge regression\n",
    "- Elastic net regularization\n",
    "- Extra trees regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1513ca82be1cad806c6c2f248f84eecca7f53883"
   },
   "outputs": [],
   "source": [
    "# importing the models\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, ElasticNet, Lasso, SGDRegressor, Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import ExtraTreesRegressor,GradientBoostingRegressor,RandomForestRegressor\n",
    "from sklearn.svm import LinearSVR,SVR\n",
    "# creating the models\n",
    "models = [\n",
    "             LinearRegression(),\n",
    "             SVR(),\n",
    "             SGDRegressor(),\n",
    "             SGDRegressor(max_iter=1000, tol = 1e-3),\n",
    "             GradientBoostingRegressor(),\n",
    "             RandomForestRegressor(),\n",
    "             Lasso(),\n",
    "             Lasso(alpha=0.01,max_iter=10000),\n",
    "             Ridge(),\n",
    "             BayesianRidge(),\n",
    "             KernelRidge(),\n",
    "             KernelRidge(alpha=0.6,kernel='polynomial',degree = 2,coef0=2.5),\n",
    "             ElasticNet(),\n",
    "             ElasticNet(alpha = 0.001,max_iter=10000),    \n",
    "             ExtraTreesRegressor(),\n",
    "             ]\n",
    "\n",
    "names = ['Linear regression','Support vector regression','Stochastic gradient descent','Stochastic gradient descent 2','Gradient boosting tree','Random forest','Lasso regression','Lasso regression 2','Ridge regression','Bayesian ridge regression','Kernel ridge regression','Kernel ridge regression 2','Elastic net regularization','Elastic net regularization 2','Extra trees regression']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "15b3bf83e45dec3ced5f9a3146e644d18dcc9147"
   },
   "outputs": [],
   "source": [
    "# Define a root mean square error function\n",
    "def rmse(model,X,y):\n",
    "    rmse = np.sqrt(-cross_val_score(model,X,y,scoring=\"neg_mean_squared_error\",cv=5))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7e04f721fa121d2b8ca4e4b02ae48119f640535"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Perform 5-folds cross-calidation to evaluate the models \n",
    "for model, name in zip(models, names):\n",
    "    # Root mean square error\n",
    "    score = rmse(model,X_train_scaled,y_train_transformed)\n",
    "    print(\"- {} : mean : {:.6f}, std : {:4f}\".format(name, score.mean(),score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a9ff7ea4b03ac805315a7d907dca974c160483e6"
   },
   "source": [
    "Surprisingly, the Random forest and Extra trees regression models are the ones who performed the worst, and the linear regression model performed actually pretty good relative to the other models.\n",
    "By compiling the above code several times and observing the different scores each time, we can classify the models by accuracy :\n",
    "\n",
    "- 1st : Kernel ridge regression\n",
    "- 2nd : Elastic net regularization and Bayesian ridge regression\n",
    "- 3rd : Ridge regression and Linear regression\n",
    "- 4rth : Support vector regression\n",
    "- 5th : Gradient boosting tree\n",
    "- 6th : Stochastic gradient  and Lasso regression\n",
    "- 7th : Random forest and Extra trees regression\n",
    "\n",
    "I think we got a good score in Elastic net regularization, Lasso regression and Stochastic gradient descent because we chose some good parameters. We can see that their score above is very bad when not specifing parameter values. So if we really want to know to best model, we need to choose optimal parameters for all the models, and tha's what we will do in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7742ffec8d885390b847a3521e4356b8b0e8cc9"
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<h1>Hyper-parameter Tuning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da2043ea79e12b40cebfe179c7d2c371137f7055"
   },
   "source": [
    "For choosing the most optimal hyper-parameters, we will perform gird search. the class GridSearchCV exhaustively considers all parameter combinations and generates candidates from a grid of parameter values specified with the param_grid parameter.\n",
    "Since we will use the same procedure for all models, we will start by creating a function which takes specified parameter values as entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d42512019e9b2d5478f02e03aa7141b88ebf029c"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class gridSearch():\n",
    "    def __init__(self,model):\n",
    "        self.model = model\n",
    "    def grid_get(self,param_grid):\n",
    "        grid_search = GridSearchCV(self.model,param_grid,cv=5,scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(X_train_scaled,y_train_transformed)\n",
    "        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n",
    "        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])\n",
    "        print('\\nBest parameters : {}, best score : {}'.format(grid_search.best_params_,np.sqrt(-grid_search.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9093f68601ce5313c4267f7824bda67cf70f55e8"
   },
   "source": [
    "1. Kernel ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "989b616e7fa58fda179454a5391b0b697e927eb3"
   },
   "outputs": [],
   "source": [
    "gridSearch(KernelRidge()).grid_get(\n",
    "        {'alpha':[3.5,4,4.5,5,5.5,6,6.5], 'kernel':[\"polynomial\"], 'degree':[3],'coef0':[1,1.5,2,2.5,3,3.5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f9a8affea5c018bfd21ac45527f28b530c2038e2"
   },
   "source": [
    "2. Elastic net regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "535c7abf5be2c5eaaf78c81bbe8df7c9381820fb"
   },
   "outputs": [],
   "source": [
    "gridSearch(ElasticNet()).grid_get(\n",
    "        {'alpha':[0.006,0.0065,0.007,0.0075,0.008],'l1_ratio':[0.070,0.075,0.080,0.085,0.09,0.095],'max_iter':[10000]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94c031f97b9143bb6e5b0a6daf2be0393e5aa3f6"
   },
   "source": [
    "3. Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "277200b6d2b72e201be8d6405eaa2cdbdf511f4b"
   },
   "outputs": [],
   "source": [
    "gridSearch(Ridge()).grid_get(\n",
    "        {'alpha':[10,20,25,30,35,40,45,50,55,57,60,65,70,75,80,100],'max_iter':[10000]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1bb3d6646185336a16e3cfa4d8a0e99824a74eb6"
   },
   "source": [
    "4. Support vector regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1db005416aba236323f9a876d630dc8abce22d84"
   },
   "outputs": [],
   "source": [
    "gridSearch(SVR()).grid_get(\n",
    "        {'C':[13,15,17,19,21],'kernel':[\"rbf\"],\"gamma\":[0.0005,0.001,0.002,0.01],\"epsilon\":[0.01,0.02,0.03,0.1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "002af4280fc79b8eec58bbda98a9021a19b533b2"
   },
   "source": [
    "5. Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f7c1d031e1ac6b94c8ed3db514ea803a9a60096"
   },
   "outputs": [],
   "source": [
    "gridSearch(Lasso()).grid_get(\n",
    "       {'alpha':[0.01,0.001,0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0008,0.0009],'max_iter':[10000]})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We see that the models perform almost the same way with a score of 0.116. Let's define these models with the their respective best hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "786675eb3cff569064c36a7975b66d24e8628bae"
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha= 0.0006, max_iter= 10000)\n",
    "ridge = Ridge(alpha=35, max_iter= 10000)\n",
    "svr = SVR(C = 13, epsilon= 0.03, gamma = 0.001, kernel = 'rbf')\n",
    "ker = KernelRidge(alpha=6.5 ,kernel='polynomial', degree=3 , coef0=2.5)\n",
    "ela = ElasticNet(alpha=0.007,l1_ratio=0.07,max_iter=10000)\n",
    "bay = BayesianRidge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2c6bdedd462190b54244c0c0c3e21e0f55657f5"
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<h1>Combining Models</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d4edb16610d4752807dbf6a9da0d65340af2b1da"
   },
   "source": [
    "In order to further improve our model accuracy. We will use an ensemble method. I chose to use stacking. Stacked generalization is an ensemble method where the models are combined using another machine learning algorithm. The basic idea is to train machine learning algorithms with training dataset and then generate a new dataset with these models. Then this new dataset is used as input for the combiner machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e5e3986672777c294f136d811c3ad9f1575daab7"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor,GradientBoostingRegressor,RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ae15dcf1c3473632a9b7f1ba86a0b1ca839859c"
   },
   "outputs": [],
   "source": [
    "# Creating the stacking function\n",
    "class stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self,mod,meta_model):\n",
    "        self.mod = mod\n",
    "        self.meta_model = meta_model\n",
    "        kff = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "        self.kf = kff\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.saved_model = [list() for i in self.mod]\n",
    "        oof_train = np.zeros((X.shape[0], len(self.mod)))\n",
    "        \n",
    "        for i,model in enumerate(self.mod):\n",
    "            for train_index, val_index in self.kf.split(X,y):\n",
    "                renew_model = clone(model)\n",
    "                renew_model.fit(X[train_index], y[train_index])\n",
    "                self.saved_model[i].append(renew_model)\n",
    "                oof_train[val_index,i] = renew_model.predict(X[val_index])\n",
    "        \n",
    "        self.meta_model.fit(oof_train,y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        whole_test = np.column_stack([np.column_stack(model.predict(X) for model in single_model).mean(axis=1) \n",
    "                                      for single_model in self.saved_model]) \n",
    "        return self.meta_model.predict(whole_test)\n",
    "    \n",
    "    def get_oof(self,X,y,test_X):\n",
    "        oof = np.zeros((X.shape[0],len(self.mod)))\n",
    "        test_single = np.zeros((test_X.shape[0],5))\n",
    "        test_mean = np.zeros((test_X.shape[0],len(self.mod)))\n",
    "        for i,model in enumerate(self.mod):\n",
    "            for j, (train_index,val_index) in enumerate(self.kf.split(X,y)):\n",
    "                clone_model = clone(model)\n",
    "                clone_model.fit(X[train_index],y[train_index])\n",
    "                oof[val_index,i] = clone_model.predict(X[val_index])\n",
    "                test_single[:,j] = clone_model.predict(test_X)\n",
    "            test_mean[:,i] = test_single.mean(axis=1)\n",
    "        return oof, test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa1393caa130820975b3463fb1162830a918d67f"
   },
   "outputs": [],
   "source": [
    "# Impute the training dataset\n",
    "X_scaled_imputed = Imputer().fit_transform(X_train_scaled)\n",
    "y_log_imputed = Imputer().fit_transform(y_train_transformed.values.reshape(-1,1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da09b1c25ebe4b28b28786ea8b135802bef77a5b"
   },
   "outputs": [],
   "source": [
    "X_scaled_imputed.shape,y_log_imputed.shape,X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90a8a6561a0b6d98a6f513b9eb9e3d41387bfd83",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculating the score\n",
    "stack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)\n",
    "score = rmse(stack_model,X_scaled_imputed,y_log_imputed)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "15c6a4a38515828cbedabc0a1e6013c59c45e26f"
   },
   "source": [
    "We obtain a score of 0.113, which is slightly better than the average score of the other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3dd418b18b865351c1ae5bb30d9eaf7ff5256bf0"
   },
   "outputs": [],
   "source": [
    "# Combining the extracted features generated from stacking whith original features\n",
    "X_train_stack,X_test_stack = stack_model.get_oof(X_scaled_imputed,y_log_imputed,X_test_scaled)\n",
    "X_train_add = np.hstack((X_scaled_imputed,X_train_stack))\n",
    "X_test_add = np.hstack((X_test_scaled,X_test_stack))\n",
    "X_train_add.shape,X_test_add.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eda34b8bf20c26efcdfa81cc58528880ad6ae0fe"
   },
   "outputs": [],
   "source": [
    "# Calculate the final score of the model\n",
    "score = rmse(stack_model,X_train_add,y_log_imputed)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7730936b15568d5db8afee81b4d351a926756a1d"
   },
   "source": [
    "The last score we obtain is 0.1074, which is quite good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f94660c118ba10b01de2e0fef21e4d104683ba6d"
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<h1>Making Predictions</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fcc367b10ed92fbbc8a333ffb8f3eedc85737f44"
   },
   "source": [
    "Now it's time to make predictions and store them in a csv file with corresponding Ids. after we make prediction we need to transform them to their original shape with exponential function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c60e656aaa311726fd3a91ddbb8922d842590025"
   },
   "outputs": [],
   "source": [
    "# Fit the model to the dataset generated with stacking\n",
    "stack_model.fit(X_train_add,y_log_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aab29c94adedc3d915f1a9299f46620a0cae2c34"
   },
   "outputs": [],
   "source": [
    "# Making prediction on the test set generated by stacking\n",
    "predicted_prices = np.exp(stack_model.predict(X_test_add))\n",
    "# Prepare the csv file\n",
    "my_submission = pd.DataFrame({'Id': testData.Id, 'SalePrice': predicted_prices})\n",
    "my_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aee13d8a2c7145d23d3e6e60c98108b598dcf716"
   },
   "source": [
    "The last time I submitted, I was the 859th (top 20%) with a score of 0.11934"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "333f01b65681799faa52a7cf8908f81f0492c748",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<h2>Thanks for reading my notebook. If you liked my kernel please kindly UPVOTE for other people to see. If you have any remarks, please leave a comment bellow.<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8037a21d35dfce30628355c7bc2e77526e0ae63c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
